"""
Quick Index & Navigation Guide
Tokopedia NLP Analysis Project
"""

# ============================================================================
# PROJECT STRUCTURE
# ============================================================================

PROJECT_STRUCTURE = """
ğŸ“ nlp_project/
â”‚
â”œâ”€â”€ ğŸ“‚ src/                           [NLP MODULES - Core Components]
â”‚   â”œâ”€â”€ sentiment_analyzer.py        â†’ Sentiment analysis (TextBlob + Lexicon)
â”‚   â”œâ”€â”€ feature_extraction.py        â†’ BoW & TF-IDF extraction
â”‚   â”œâ”€â”€ text_classifier.py           â†’ Classification (4 algorithms)
â”‚   â”œâ”€â”€ modeling_engine.py           â†’ Hyperparameter tuning
â”‚   â”œâ”€â”€ named_entity_recognition.py  â†’ NER & POS tagging
â”‚   â”œâ”€â”€ visualization.py             â†’ Charts & reports
â”‚   â””â”€â”€ data_processor.py            â†’ Excel/CSV I/O
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                       [EXECUTABLE SCRIPTS]
â”‚   â”œâ”€â”€ main.py                      â†’ Full NLP pipeline (run this!)
â”‚   â”œâ”€â”€ dashboard.py                 â†’ Interactive Streamlit dashboard
â”‚   â”œâ”€â”€ analyze_tokopedia.py         â†’ Full analysis with all features
â”‚   â””â”€â”€ analyze_tokopedia_simple.py  â†’ Lightweight analysis
â”‚
â”œâ”€â”€ ğŸ“‚ data/                          [INPUT DATA]
â”‚   â””â”€â”€ Dataset pengguna tokopedia.csv â†’ 1,999 user reviews
â”‚
â”œâ”€â”€ ğŸ“‚ output/                        [RESULTS & OUTPUTS]
â”‚   â”œâ”€â”€ *.txt                        â†’ Text reports
â”‚   â””â”€â”€ *.xlsx                       â†’ Excel results
â”‚
â”œâ”€â”€ ğŸ“„ README.md                      â†’ Full documentation
â”œâ”€â”€ ğŸ“„ config.json                   â†’ Configuration file
â””â”€â”€ ğŸ“„ TOKOPEDIA_ANALYSIS_SUMMARY.txt â†’ Analysis summary
"""

# ============================================================================
# QUICK START COMMANDS
# ============================================================================

QUICK_START = """
ğŸš€ QUICK START

1. Run Full Pipeline Analysis:
   $ cd scripts
   $ python main.py

2. View Interactive Dashboard:
   $ cd scripts
   $ streamlit run dashboard.py
   
3. Simple Analysis (No Heavy Dependencies):
   $ cd scripts
   $ python analyze_tokopedia_simple.py

4. Read Documentation:
   $ Open: README.md
"""

# ============================================================================
# MODULE DESCRIPTIONS
# ============================================================================

MODULES = {
    'sentiment_analyzer.py': {
        'description': 'Sentiment Analysis Module',
        'key_classes': ['SentimentAnalyzer'],
        'key_methods': [
            'analyze_sentiment_textblob()',
            'analyze_sentiment_lexicon()',
            'analyze_sentiment_combined()',
            'analyze_batch()',
            'get_sentiment_distribution()'
        ],
        'features': [
            'TextBlob polarity/subjectivity',
            'Lexicon-based classification',
            'Positive/negative word detection',
            'Intensifier & negator handling',
            'Combined sentiment prediction'
        ]
    },
    
    'feature_extraction.py': {
        'description': 'Feature Extraction Module',
        'key_classes': ['TextFeatureExtractor'],
        'key_methods': [
            'extract_bow()',
            'extract_tfidf()',
            'extract_word_frequency()',
            'extract_bigrams()',
            'extract_text_statistics()',
            'get_top_features()'
        ],
        'features': [
            'Bag of Words (BoW)',
            'TF-IDF vectorization',
            'Word frequency analysis',
            'Bigram extraction',
            'Text length statistics',
            'Vocabulary analysis'
        ]
    },
    
    'text_classifier.py': {
        'description': 'Text Classification Module',
        'key_classes': ['TextClassifier'],
        'algorithms': [
            'Multinomial Naive Bayes',
            'Linear SVM',
            'Logistic Regression',
            'Random Forest'
        ],
        'key_methods': [
            'train_naive_bayes()',
            'train_svm()',
            'train_logistic_regression()',
            'train_random_forest()',
            'train_all_models()',
            'predict()',
            'get_results_summary()'
        ]
    },
    
    'modeling_engine.py': {
        'description': 'Model Optimization Module',
        'key_classes': ['ModelingEngine'],
        'key_methods': [
            'tune_naive_bayes()',
            'tune_svm()',
            'tune_logistic_regression()',
            'tune_random_forest()',
            'tune_all_models()',
            'cross_validate()',
            'get_tuning_results_summary()'
        ],
        'features': [
            'GridSearchCV hyperparameter tuning',
            'Cross-validation (k-fold)',
            'Multiple scoring metrics',
            'Best model selection'
        ]
    },
    
    'named_entity_recognition.py': {
        'description': 'NER & POS Tagging Module',
        'key_classes': [
            'NamedEntityRecognizer',
            'POS_Tagger',
            'EntitySentimentAnalyzer'
        ],
        'entities': [
            'PERSON', 'ORG', 'GPE', 'DATE', 'LOCATION', 'etc'
        ],
        'key_methods': [
            'extract_entities()',
            'extract_entities_batch()',
            'tag_pos()',
            'extract_by_pos()',
            'get_entity_distribution()',
            'analyze_entity_sentiment()'
        ]
    },
    
    'visualization.py': {
        'description': 'Visualization & Reporting Module',
        'key_classes': [
            'VisualizationHelper',
            'AnalysisReporter'
        ],
        'visualization_types': [
            'Sentiment distribution (pie, bar)',
            'Word frequency plots',
            'Confusion matrices',
            'Text length distributions',
            'Model metrics comparison'
        ],
        'report_types': [
            'Text reports (.txt)',
            'Excel reports (.xlsx)',
            'Summary statistics'
        ]
    },
    
    'data_processor.py': {
        'description': 'Data Processing Module',
        'key_classes': [
            'ExcelDataHandler',
            'DataProcessor'
        ],
        'key_methods': [
            'load_excel()',
            'save_to_excel()',
            'load_csv()',
            'apply_styling()',
            'clean_data()'
        ]
    }
}

# ============================================================================
# PIPELINE FLOW
# ============================================================================

PIPELINE_FLOW = """
ğŸ“Š NLP PIPELINE - 6 STAGES

[1] LOAD DATA
    â””â”€ Load CSV dataset (1,999 reviews)

[2] SENTIMENT ANALYSIS
    â”œâ”€ TextBlob analysis
    â”œâ”€ Lexicon-based sentiment
    â””â”€ Distribution calculation

[3] FEATURE EXTRACTION
    â”œâ”€ Word frequency
    â”œâ”€ Bigram extraction
    â”œâ”€ TF-IDF vectorization
    â””â”€ Text statistics

[4] NAMED ENTITY RECOGNITION (Optional)
    â”œâ”€ Entity extraction
    â”œâ”€ POS tagging
    â””â”€ Entity sentiment analysis

[5] TEXT CLASSIFICATION & MODELING
    â”œâ”€ TF-IDF vectorization
    â”œâ”€ Train 4 classifiers
    â”œâ”€ Evaluate metrics
    â””â”€ Hyperparameter tuning

[6] GENERATE REPORTS
    â”œâ”€ Text reports (.txt)
    â”œâ”€ Excel files (.xlsx)
    â””â”€ Dashboard visualization
"""

# ============================================================================
# KEY FILES & WHAT THEY DO
# ============================================================================

KEY_FILES = {
    'main.py': 'Main NLP pipeline - orchestrates all components',
    'dashboard.py': 'Interactive Streamlit dashboard - visualize results',
    'analyze_tokopedia.py': 'Full analysis using all modules',
    'analyze_tokopedia_simple.py': 'Lightweight analysis (minimal deps)',
    'README.md': 'Complete documentation',
    'config.json': 'Project configuration',
    'TOKOPEDIA_ANALYSIS_SUMMARY.txt': 'Analysis summary & findings'
}

# ============================================================================
# OUTPUT FILES
# ============================================================================

OUTPUT_FILES = {
    '00_Tokopedia_Analysis_Report.txt': 'Initial analysis report',
    '01_Tokopedia_Sentiment_Analysis.xlsx': 'Per-review sentiment results',
    '02_Tokopedia_Word_Frequency.xlsx': 'Top 200 most frequent words',
    '03_Tokopedia_Bigram_Frequency.xlsx': 'Top 200 most frequent bigrams',
    'NLP_Analysis_Report.txt': 'Complete pipeline analysis report',
    'NLP_Analysis_Report.xlsx': 'Multi-sheet Excel report'
}

# ============================================================================
# ANALYSIS RESULTS SUMMARY
# ============================================================================

RESULTS_SUMMARY = """
ğŸ“ˆ ANALYSIS RESULTS

Dataset: 1,999 Tokopedia user reviews

Sentiment Distribution:
â”œâ”€ POSITIF (53.38%): 1,067 reviews âœ…
â”œâ”€ NETRAL (28.26%): 565 reviews ğŸ˜
â””â”€ NEGATIF (18.36%): 367 reviews âŒ

Text Statistics:
â”œâ”€ Unique words: 3,810
â”œâ”€ Unique bigrams: 200+
â”œâ”€ Average text length: 12.09 words
â””â”€ Vocabulary richness: High

Top Keywords:
1. tokopedia (288x)
2. bisa (274x)
3. saya (263x)
4. tidak (224x)
5. promo (210x)

Top Phrases:
1. pengguna baru (51x)
2. tidak bisa (50x)
3. kurir rekomendasi (42x)
4. gak bisa (41x)
5. gratis ongkir (40x)
"""

# ============================================================================
# IMPORT PATHS
# ============================================================================

IMPORT_GUIDE = """
ğŸ”— HOW TO IMPORT MODULES

From scripts/:
  from sys import path
  path.insert(0, '../src')
  
  from sentiment_analyzer import SentimentAnalyzer
  from feature_extraction import TextFeatureExtractor
  from text_classifier import TextClassifier
  # ... etc

From main.py (already configured):
  # Just use: from sentiment_analyzer import SentimentAnalyzer
"""

# ============================================================================
# TROUBLESHOOTING
# ============================================================================

TROUBLESHOOTING = """
ğŸ”§ TROUBLESHOOTING

1. ModuleNotFoundError (import errors)
   â†’ Make sure you're in correct directory
   â†’ Check sys.path.insert(0, '../src') is present

2. FileNotFoundError (data/output not found)
   â†’ Check paths use: ../data/ and ../output/
   â†’ Run scripts from scripts/ folder only

3. Missing dependencies
   â†’ pip install pandas numpy scikit-learn textblob spacy matplotlib seaborn openpyxl streamlit
   â†’ python -m spacy download en_core_web_sm

4. Dashboard not loading
   â†’ streamlit run dashboard.py
   â†’ Check http://localhost:8501

5. Excel files empty
   â†’ Ensure openpyxl is installed
   â†’ Check output/ folder has write permissions
"""

# ============================================================================
# USEFUL COMMANDS
# ============================================================================

COMMANDS = """
ğŸ“‹ USEFUL COMMANDS

# Navigate to project
cd nlp_project

# Run main analysis
cd scripts
python main.py

# Start dashboard
cd scripts
streamlit run dashboard.py

# Run simple analysis
cd scripts
python analyze_tokopedia_simple.py

# Check dependencies
pip list | grep -E "pandas|scikit-learn|streamlit"

# View file structure
dir /s  (Windows)
ls -R   (macOS/Linux)

# View analysis results
cat ../output/NLP_Analysis_Report.txt

# Clear output cache
rm -rf __pycache__
rm -rf .streamlit
"""

# ============================================================================
# PRINT GUIDE
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("TOKOPEDIA NLP ANALYSIS - INDEX & NAVIGATION GUIDE")
    print("=" * 80)
    print()
    print(PROJECT_STRUCTURE)
    print("\n" + "=" * 80)
    print("QUICK START")
    print("=" * 80)
    print(QUICK_START)
    print("\n" + "=" * 80)
    print("PIPELINE FLOW")
    print("=" * 80)
    print(PIPELINE_FLOW)
    print("\n" + "=" * 80)
    print("RESULTS SUMMARY")
    print("=" * 80)
    print(RESULTS_SUMMARY)
    print("\n" + "=" * 80)
    print("For detailed information, see: README.md")
    print("=" * 80)
